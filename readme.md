ğŸ“˜ Blog RAG Chat Application â€” README
A smart Retrieval-Augmented Generation (RAG)â€“based chat application that allows users to upload blogs, PDFs, text files, or URLs, and ask contextual questions.
The app uses LangChain, FAISS, HuggingFace Embeddings, and OpenAI GPT models to generate accurate answers based entirely on the uploaded content.

ğŸš€ Features

Upload multiple content types:

ğŸŒ Web URL

ğŸ“„ PDF

ğŸ“ Text File

âœï¸ Manual Text

Intelligent document chunking using Recursive Text Splitter

Semantic search using FAISS Vector Store

Context-aware answers using OpenAI GPT LLM

Clean chat-based UI using Streamlit

End-to-end RAG pipeline with retrieval + generation

Modular code structure (loaders, pipeline, UI, app controller)

ğŸ—ï¸ Tech Stack

Frontend: Streamlit
Backend: Python
AI Framework: LangChain
Embeddings: Sentence Transformers (MiniLM)
Vector Store: FAISS
LLM Provider: OpenAI GPT Models
Document Loaders: WebBaseLoader, PyPDFLoader, TXT Loader
Environment Config: python-dotenv
Testing Tools: Streamlit logs, Postman (optional)

ğŸ§  High-Level Architecture
User
  â”‚
  â–¼
Streamlit UI
  â”‚
  â–¼
Application Controller
  â”‚
  â–¼
Document Loaders (URL/PDF/TXT/Manual)
  â”‚
  â–¼
Text Splitter (Chunking)
  â”‚
  â–¼
Embeddings Generator (MiniLM)
  â”‚
  â–¼
FAISS Vector Store
  â”‚
  â–¼
Retriever (Top-K Chunks)
  â”‚
  â–¼
LLM (OpenAI GPT Model)
  â”‚
  â–¼
RAG Answer Generator
  â”‚
  â–¼
Streamlit UI â€” Final Answer

ğŸ§© Low-Level Design (Modules)
1ï¸âƒ£ Loaders Module

Handles all input types

load_from_url()

load_from_pdf()

load_from_text_file()

load_from_manual_text()

2ï¸âƒ£ Vector Store Module

Split documents into chunks

Embed using MiniLM

Store vectors inside FAISS

Return retriever

3ï¸âƒ£ RAG Pipeline

RetrievalQA

PromptTemplate

OpenAI GPT LLM

Answer generation

4ï¸âƒ£ UI Module

Chat layout

Message bubbles

Sidebar upload panel

5ï¸âƒ£ App Controller

Session state

Connecting UI â†’ Pipeline â†’ Output

ğŸ“‚ Project Structure
ğŸ“¦ Blog-RAG-Chat-App
â”‚
â”œâ”€â”€ app.py                # Main Streamlit app
â”œâ”€â”€ pipeline.py           # RAG pipeline (Retrieval + LLM)
â”œâ”€â”€ loaders.py            # URL/PDF/Text loaders
â”œâ”€â”€ ui.py                 # Chat UI components
â”œâ”€â”€ config.py             # API keys & model configs
â”œâ”€â”€ .env                  # OpenAI Key
â””â”€â”€ requirements.txt      # Dependencies

âš™ï¸ Installation & Setup
1. Clone the Repository
git clone https://github.com/yourusername/blog-rag-chat.git
cd blog-rag-chat

2. Create Virtual Environment
python -m venv venv
source venv/bin/activate       # Mac/Linux
venv\Scripts\activate          # Windows

3. Install Dependencies
pip install -r requirements.txt

4. Add Your OpenAI Key

Create .env:

OPENAI_API_KEY=your_key_here

5. Run App
streamlit run app.py

ğŸ“¦ Example Usage

Upload a PDF / URL / Text

Ask questions like:

â€œSummarize this blogâ€

â€œWhat are the key points?â€

â€œExplain section 3 in simple wordsâ€

â€œWhat does the author say about AI ethics?â€

Get contextual answers based only on your document.
ğŸ”® Future Scope

Persistent vector storage using ChromaDB/Pinecone

Multi-file combined knowledge retrieval

Chat history memory

Add authentication + user sessions

Deploy on cloud (Railway / Render / AWS)

Add a React frontend + FastAPI backend version

ğŸ¤ Contributing

Pull requests are welcome.
For major changes, please open an issue first to discuss what you'd like to change.

ğŸ“œ License

This project is licensed under MIT License.

ğŸ™Œ Acknowledgements

LangChain

Streamlit

HuggingFace

OpenAI